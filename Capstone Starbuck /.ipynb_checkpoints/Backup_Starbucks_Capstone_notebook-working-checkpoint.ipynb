{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This data set contains simulated data that mimics customer behavior on the Starbucks rewards mobile app. Once every few days, Starbucks sends out an offer to users of the mobile app. An offer can be merely an advertisement for a drink or an actual offer such as a discount or BOGO (buy one get one free). Some users might not receive any offer during certain weeks. \n",
    "\n",
    "Not all users receive the same offer, and that is the challenge to solve with this data set.\n",
    "\n",
    "Your task is to combine transaction, demographic and offer data to determine which demographic groups respond best to which offer type. This data set is a simplified version of the real Starbucks app because the underlying simulator only has one product whereas Starbucks actually sells dozens of products.\n",
    "\n",
    "Every offer has a validity period before the offer expires. As an example, a BOGO offer might be valid for only 5 days. You'll see in the data set that informational offers have a validity period even though these ads are merely providing information about a product; for example, if an informational offer has 7 days of validity, you can assume the customer is feeling the influence of the offer for 7 days after receiving the advertisement.\n",
    "\n",
    "You'll be given transactional data showing user purchases made on the app including the timestamp of purchase and the amount of money spent on a purchase. This transactional data also has a record for each offer that a user receives as well as a record for when a user actually views the offer. There are also records for when a user completes an offer. \n",
    "\n",
    "Keep in mind as well that someone using the app might make a purchase through the app without having received an offer or seen an offer.\n",
    "\n",
    "### Example\n",
    "\n",
    "To give an example, a user could receive a discount offer buy 10 dollars get 2 off on Monday. The offer is valid for 10 days from receipt. If the customer accumulates at least 10 dollars in purchases during the validity period, the customer completes the offer.\n",
    "\n",
    "However, there are a few things to watch out for in this data set. Customers do not opt into the offers that they receive; in other words, a user can receive an offer, never actually view the offer, and still complete the offer. For example, a user might receive the \"buy 10 dollars get 2 dollars off offer\", but the user never opens the offer during the 10 day validity period. The customer spends 15 dollars during those ten days. There will be an offer completion record in the data set; however, the customer was not influenced by the offer because the customer never viewed the offer.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "This makes data cleaning especially important and tricky.\n",
    "\n",
    "You'll also want to take into account that some demographic groups will make purchases even if they don't receive an offer. From a business perspective, if a customer is going to make a 10 dollar purchase without an offer anyway, you wouldn't want to send a buy 10 dollars get 2 dollars off offer. You'll want to try to assess what a certain demographic group will buy when not receiving any offers.\n",
    "\n",
    "### Final Advice\n",
    "\n",
    "Because this is a capstone project, you are free to analyze the data any way you see fit. For example, you could build a machine learning model that predicts how much someone will spend based on demographics and offer type. Or you could build a model that predicts whether or not someone will respond to an offer. Or, you don't need to build a machine learning model at all. You could develop a set of heuristics that determine what offer you should send to each customer (i.e., 75 percent of women customers who were 35 years old responded to offer A vs 40 percent from the same demographic to offer B, so send offer A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets\n",
    "\n",
    "The data is contained in three files:\n",
    "\n",
    "* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n",
    "* profile.json - demographic data for each customer\n",
    "* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n",
    "\n",
    "Here is the schema and explanation of each variable in the files:\n",
    "\n",
    "**portfolio.json**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings)\n",
    "\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "**transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours since start of test. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record\n",
    "\n",
    "**Note:** If you are using the workspace, you will need to go to the terminal and run the command `conda update pandas` before reading in the files. This is because the version of pandas in the workspace cannot read in the transcript.json file correctly, but the newest version of pandas can. You can access the termnal from the orange icon in the top left of this notebook.  \n",
    "\n",
    "You can see how to access the terminal and how the install works using the two images below.  First you need to access the terminal:\n",
    "\n",
    "<img src=\"pic1.png\"/>\n",
    "\n",
    "Then you will want to run the above command:\n",
    "\n",
    "<img src=\"pic2.png\"/>\n",
    "\n",
    "Finally, when you enter back into the notebook (use the jupyter icon again), you should be able to run the below cell without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: progressbar in /Users/akbaralishaikh/opt/anaconda3/lib/python3.7/site-packages (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, json, re, os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from clean_data import * \n",
    "from exploratory_data_analysis import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from JSON file\n",
    "\n",
    "* Check dataframe and familiarise with data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the json files\n",
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.shape, profile.shape, transcript.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.info() # No missing values in portfolio dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.info() # missing values in profile dataframe (gender and income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.shape[0] - profile.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile dataframe has missing values (gender and income) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.info() # No missing values in transcript dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offer master database \n",
    "portfolio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer master \n",
    "profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = clean_portfolio()\n",
    "portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.isnull().sum(axis=0) * 100 / profile.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '%' sex split where gender is specified\n",
    "profile['gender'].value_counts()*100/ profile['gender'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '%' sex split as per overall profile count \n",
    "profile['gender'].value_counts()*100/ profile.count()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Compute summary customer profile attribute statistics when customer income is not specified </b>\n",
    "* Some customer Income is not specified and some customer Age is specified as 118, looks high than normal - Needs an investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile[profile['income'].isnull()][['age', 'income']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Compute summary customer profile attribute statistics when customer income is specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile[profile['income'].notnull()][['age', 'income']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Evaluate what year a customer became a rewards member </b>\n",
    "    \n",
    "* Over the years customer membership has increase except in 2018 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert string date to date and time and check membership trend\n",
    "became_member_on = profile['became_member_on'].apply(convert_to_datetime)\n",
    "\n",
    "start_year = became_member_on.apply(lambda elem: elem.year).value_counts()\n",
    "start_year *= 100 / start_year.sum()\n",
    "start_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluate which month a customer became a rewards member</b>\n",
    "\n",
    "* membership registration between month of Aug and Jan higher than in other months, very well looks to be a seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_month = became_member_on.apply(lambda elem: elem.month).value_counts()\n",
    "start_month *= 100 / start_month.sum()\n",
    "start_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the customer profile data\n",
    "\n",
    "\n",
    "* Remove customers details with missing income and gender  is missing\n",
    "* Change the name of the 'id' column to 'customerid'\n",
    "* Transform the 'became_member_on' column to a datetime object\n",
    "* One hot encode a customer's membership start year\n",
    "* One hot encode a customer's age range\n",
    "* Transform a customer's gender from a character to a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(profile['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='age', y ='income', data=profile, kind='kde', dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(profile,\n",
    " gender_integer_map) = clean_profile()\n",
    "\n",
    "print(\"Number of user profiles: %d\" % (profile.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print the first five rows of the preprocessed (i.e. clean) customer profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check year wise / gender registration\n",
    "\n",
    "#chk_gender_year = profile.groupby('gender')[[2014, 2015, 2016, 2017, 2018]].sum()\n",
    "#chk_gender_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.pivot_table(profile, index=[2014, 2015, 2016, 2017, 2018], columns='gender', \n",
    "#                    values='income', aggfunc=np.mean, dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile.groupby('gender')[2014, 2015, 2016, 2017, 2018, 'income'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print the mapping of customer gender string to an integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(profile.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Plot Income Distribution as a Function of Gender </b>\n",
    "    \n",
    "Results suggest that the minimum income for both male and female customer are same, whereas average income of male is lower than female, but female income have few outliers on higher side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot('gender', profile['income']*1E-3, data = profile)\n",
    "plt.xlabel('Income [10K]')\n",
    "plt.ylabel('P(Income)')\n",
    "plt.legend(gender_integer_map)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_customers = profile[profile['gender'] == 1]\n",
    "female_customers = profile[profile['gender'] == 0]\n",
    "\n",
    "current_palette = sns.color_palette()\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('white')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4), nrows=1, ncols=2,\n",
    "                       sharex=True, sharey=True)\n",
    "\n",
    "plt.sca(ax[0])\n",
    "sns.distplot(male_customers['income'] * 1E-3,\n",
    "             color=current_palette[4], kde=True)\n",
    "plt.xlabel('Income [10K]')\n",
    "plt.ylabel('P(Income)')\n",
    "plt.title('Male Customer Income')\n",
    "\n",
    "plt.sca(ax[1])\n",
    "sns.distplot(female_customers['income'] * 1E-3, \n",
    "             color=current_palette[2], kde=True)\n",
    "plt.xlabel('Income [10K]')\n",
    "plt.ylabel('P(Income)')\n",
    "plt.title('Female Customer Income')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Visualise Membership Start Year Statistics </b>\n",
    "\n",
    "The results suggest that most customers recently joined the Starbucks rewards program. These results also suggest that there are more male customers than female customers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "membership_date = initialize_membership_date(profile, gender_integer_map)\n",
    "\n",
    "sns.barplot(x='startyear', y='count', hue='gender', data=membership_date, capsize=.2)\n",
    "plt.xlabel('Membership Start Year')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Check customer gender distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(membership_date.groupby('gender')['count'].sum() / membership_date['count'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Age Range Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_range = init_agerange(profile,\n",
    "                          gender_integer_map)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x='agerange', y='count', hue='gender', data=age_range)\n",
    "plt.xlabel('Age Range')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(age_range.groupby('agerange')['count'].sum() / age_range['count'].sum()).map(lambda n: '{:,.2%}'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(age_range.groupby(['agerange','gender'])['count'].sum() / age_range['count'].sum()).map(lambda n: '{:,.2%}'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (age_range.pivot(index='agerange', columns='gender', values='count')\n",
    "         .reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Transaction data\n",
    "\n",
    "* Customer transaction record attributes\n",
    " * event (str) - Record description (i.e. transaction, offer received, offer viewed, etc.)\n",
    " * person (str) - Customer id\n",
    " * time (int) - Time in hours. The data begins at time t=0\n",
    " * value - (dict of strings) - Either an offer id or transaction amount depending on the record\n",
    "\n",
    "* Customer transaction data EDA conclusions\n",
    " * Need to separate offer and customer purchase data\n",
    " * Results suggest ~ 45 % of the events are customers purchases and ~ 55% of the events describe customer offers\n",
    "\n",
    "<b>Print the first five rows of the transaction data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Compute the event type distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript['event'].value_counts() / transcript['event'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(transcript['event'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_counts = transcript['event'].value_counts()\n",
    "event_counts = pd.DataFrame(list(zip(event_counts.index.values, event_counts)),\n",
    "                            columns=['event', 'count'])\n",
    "event_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Compute the percentage of customer transaction and offer events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transactions = event_counts['count'].sum()\n",
    "\n",
    "percentage_transactions = 100 * event_counts.iloc[0]['count'] / total_transactions\n",
    "percentage_offers = 100 * event_counts.iloc[1:]['count'].sum() / total_transactions\n",
    "\n",
    "print(\"Percentage of customer transaction events: %.1f %%\" % (percentage_transactions))\n",
    "print(\"Percentage of customer offer events: %.1f %%\" % (percentage_offers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the transaction data\n",
    "\n",
    "1. Rename 'person' column to 'customerid' and remove customer details that are not present in customer profile dataframe\n",
    "2. Convert time variable units from hours to days\n",
    "3. Change the name of the 'time' column to 'timedays'\n",
    "4. Create a DataFrame that describes offers\n",
    " * Create an offerid column\n",
    " * Parse the offer event type (i.e. 'received', 'viewed', or 'completed')\n",
    " * One hot encode customer offer events\n",
    "5. Create a DataFrame that describes customer transaction events\n",
    " * Parse customer transaction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_data, transaction = clean_transcript(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print the first five rows of the transformed customer transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine transaction, demographic and offer data\n",
    "\n",
    "For each customer, apply the following algorithm:\n",
    "\n",
    "1. join customer's profile, with offer and transaction\n",
    "2. Initialize DataFrames that describe when a customer receives, views, and completes an offer\n",
    "3. Iterate over each offer a customer receives\n",
    " * Initialize the current offer id\n",
    " * Look-up a description of the current offer\n",
    " * Initialize the time period when an offer is valid\n",
    " * Initialize a Boolean array that select customer transactions that fall within the valid offer time window\n",
    " * Initialize a Boolean array that selects a description of when a customer completes an offer (this array may not contain any True values)\n",
    " * Initialize a Boolean array that selects a description of when a customer views an offer (this array may not contain any True values)\n",
    " * Determine whether the current offer was successful\n",
    "  * For an offer to be successful a customer has to view and complete it\n",
    " * Select customer transactions that occurred within the current offer valid time window\n",
    " * Initialize a dictionary that describes the current customer offer\n",
    " * Update a list of dictionaries that describes the effectiveness of offers to a specific customer\n",
    "\n",
    "\n",
    "Once all customer transactions have been evaluated, convert the resulting list of dictionaries into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_csvfile = \"clean_data.csv\"\n",
    "\n",
    "if os.path.exists(clean_data_csvfile):\n",
    "    clean_data = pd.read_csv(clean_data_csvfile)\n",
    "    column_ordering = ['time', 'offerid', 'customerid', 'totalamount',\n",
    "                   'offersuccessful', 'difficulty', 'durationdays',\n",
    "                   'reward', 'bogo', 'discount', 'informational',\n",
    "                   'email', 'mobile', 'social', 'web', 'gender',\n",
    "                   'income', '2013', '2014', '2015', '2016', '2017', '2018',\n",
    "                    '[10, 20)', '[20, 30)',\n",
    "                    '[30, 40)', '[40, 50)','[50, 60)', '[60, 70)',\n",
    "                    '[70, 80)', '[80, 90)','[90, 100)', '[100, 110)']\n",
    "else:\n",
    "    customerid_list = offer_data['customerid'].unique()\n",
    "    cnter = 0\n",
    "    clean_data = []\n",
    "    bar = progressbar.ProgressBar(maxval=len(customerid_list), \n",
    "                widgets=[progressbar.Timer(), progressbar.Bar('='), ' ',  \n",
    "                         progressbar.Percentage(), ' ', progressbar.ETA()])\n",
    "    bar.start()\n",
    "    for idx in range(len(customerid_list)):\n",
    "        clean_data.extend(create_combined_records(customerid_list[idx],\n",
    "                                                      portfolio, profile,\n",
    "                                                      offer_data, transaction))\n",
    "        cnter+=1 \n",
    "        bar.update(cnter)\n",
    "\n",
    "    bar.finish()\n",
    "    \n",
    "    column_ordering = ['time', 'offerid', 'customerid', 'totalamount',\n",
    "                   'offersuccessful', 'difficulty', 'durationdays',\n",
    "                   'reward', 'bogo', 'discount', 'informational',\n",
    "                   'email', 'mobile', 'social', 'web', 'gender',\n",
    "                   'income', 2013, 2014, 2015, 2016, 2017, 2018,\n",
    "                    '[10, 20)', '[20, 30)',\n",
    "                    '[30, 40)', '[40, 50)','[50, 60)', '[60, 70)',\n",
    "                    '[70, 80)', '[80, 90)','[90, 100)', '[100, 110)']\n",
    "\n",
    "#\n",
    "\n",
    "clean_data = pd.DataFrame(clean_data)\n",
    "\n",
    "# Initialize a list that describes the desired output DataFrame\n",
    "# column ordering\n",
    "\n",
    "clean_data = clean_data[column_ordering]\n",
    "\n",
    "clean_data = clean_data.sort_values('time')\n",
    "\n",
    "clean_data.to_csv(clean_data_csvfile, index=False)\n",
    "\n",
    "clean_data = clean_data.drop(columns=['time',\n",
    "                                      'customerid',\n",
    "                                      'email',\n",
    "                                      'informational'])\n",
    "\n",
    "column_ordering = ['offerid', 'totalamount']\n",
    "column_ordering.extend([elem for elem in clean_data.columns if elem not in column_ordering])\n",
    "\n",
    "clean_data = clean_data[column_ordering]\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(clean_data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above correlation plot suggest there are few positive influcing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_name = 'offersuccessful'\n",
    "random_state = 101\n",
    "variables = clean_data.drop(columns=[class_label_name])\n",
    "class_label = clean_data.filter([class_label_name])\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(variables.values,\n",
    "                            class_label.values,\n",
    "                            test_size=0.3,\n",
    "                            random_state=random_state)\n",
    "\n",
    "variable_names = variables.columns[2:]\n",
    "\n",
    "offerid_train = X_train[:, 0]\n",
    "totalamount_train = X_train[:, 1]\n",
    "X_train = X_train[:, 2:].astype('float64')\n",
    "\n",
    "offerid_test = X_test[:, 0]\n",
    "totalamount_test = X_test[:, 1]\n",
    "X_test = X_test[:, 2:].astype('float64')\n",
    "\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print offer effectiveness data attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize a DataFrame that describes the training data (customer data only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_variables = pd.DataFrame(X_train, columns=variable_names)\n",
    "training_labels = pd.DataFrame(y_train, columns=[class_label_name])\n",
    "training_offerid = pd.DataFrame(offerid_train, columns=['offerid'])\n",
    "\n",
    "training_data = pd.concat([training_offerid, training_variables,\n",
    "                           training_labels], axis=1)\n",
    "\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Offer Statistics\n",
    "\n",
    "These results suggest that distribution of offers in the simulated Starbucks mobile application data is relatively uniform.\n",
    "\n",
    "Offer sucess is ranging between 6.188% to 74.669% with a mean of 46.942\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_success = initialize_percent_success(portfolio, training_data)\n",
    "percent_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_success['percentsuccess'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "sns.heatmap(percent_success.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a plot that illustrates:\n",
    "\n",
    "* How many customers were provided a specific offer\n",
    "* Offer sucess rate (% success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 4), nrows=1, ncols=2)\n",
    "\n",
    "ax[0].bar(percent_success.index + 1, percent_success['count'])\n",
    "ax[0].set_xticks(np.arange(0,10) + 1)\n",
    "ax[0].set_xlabel('Offer #')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "ax[1].plot(percent_success.index + 1,\n",
    "           percent_success['percentsuccess'],\n",
    "           linewidth=3)\n",
    "ax[1].set_xticks(np.arange(0,10) + 1)\n",
    "ax[1].set_xlabel('Offer #')\n",
    "ax[1].set_ylabel('Percent Success')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform variables\n",
    "\n",
    "* Dataset includes both numeric and one hot encoded categorical variables. Need to apply minimum / maximum scaling to numeric variables to avoid model bias. \n",
    "  * Use scikit learn feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_scale = ['difficulty', 'durationdays', 'reward', 'income']\n",
    "\n",
    "min_max_scaler = {}\n",
    "\n",
    "for idx in range(len(variables_to_scale)):\n",
    "    column_idx = np.argwhere(variable_names == variables_to_scale[idx])[0, 0]\n",
    "    \n",
    "    cur_column = variables_to_scale[idx]\n",
    "    \n",
    "    min_max_scaler[cur_column] = MinMaxScaler()\n",
    "    min_max_scaler[cur_column].fit(X_train[:, column_idx].reshape(-1, 1))\n",
    "\n",
    "    transformed_values =\\\n",
    "        min_max_scaler[cur_column].transform(X_train[:, column_idx].reshape(-1, 1))\n",
    "\n",
    "    X_train[:, column_idx] = transformed_values.reshape(1, -1)\n",
    "    \n",
    "    transformed_values =\\\n",
    "        min_max_scaler[cur_column].transform(X_test[:, column_idx].reshape(-1, 1))\n",
    "\n",
    "    X_test[:, column_idx] = transformed_values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick data check, cleanup and Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_corr_Check = pd.get_dummies(clean_data['offerid'], drop_first=True)\n",
    "EDA_corr_Check = pd.concat([clean_data, EDA_corr_Check], axis=1)\n",
    "EDA_corr_Check.drop(['offerid'], axis=1, inplace=True)\n",
    "EDA_corr_Check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,8))\n",
    "sns.heatmap(EDA_corr_Check.corr()>.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer Statistics\n",
    "\n",
    "- General trends\n",
    " * Offer success increases with average customer income and age\n",
    "  * majority of customer is in the age group of 20 and 80 amd majority between 40 and 70. This trend is much understood with income and working population trend\n",
    " * For unsuccessful offers the percentage of male customers is typically significantly higher than the percentage of female customers\n",
    " * Offers are less successful for customers who created an account on the Starbucks rewards mobile application in 2018 (membership start year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>offerid: fafdcd668e3743c1bb461111dcafc2a4 </b>\n",
    "* Offer successful\n",
    "  - Average customer income: 66281.1\n",
    "  - Average customer age: 55.9 years\n",
    "  - Percentage of male customers: 54.47%\n",
    "  - Percentage of female customers: 45.52%\n",
    "\n",
    "* Offer unsuccessful\n",
    "  - Average customer income: 57559,3\n",
    "  - Average customer age: 52.9 years\n",
    "  - Percentage of male customers: 69.47%\n",
    "  - Percentage of female customers: 30.52%\n",
    "\n",
    "<b><u>Summary</u>:</b> 2015, 2016 and 2017 members are more likely to opt for offer, whereas 2018 joined members are more inclined to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(0, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: 2298d6c36e964ae4a3e7e9706d1fb8c2 </b>\n",
    "\n",
    "- Offer successful\n",
    " - Average customer income: 65693.0\n",
    " - Average customer age: 55.4 years\n",
    " - Percentage of male customers: 54.50%\n",
    " - Percentage of female customers: 45.49%\n",
    " \n",
    "- Offer unsuccessful\n",
    " - Average customer income: 59768.6\n",
    " - Average customer age: 53 years\n",
    " - Percentage of male customers: 69.64%\n",
    " - Percentage of female customers: 30.35%\n",
    "\n",
    "<b><u>Summary</u>:</b> 2015, 2016 and 2017 joined members are likely to opt for the offer, wheras for 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(1, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: f19421c1d4aa40978ebb69ca19b0e20d </b>\n",
    "- Offer successful\n",
    " - Average customer income: 67313.8\n",
    " - Average customer age: 56.5 years\n",
    " - Percentage of male customers: 52.28%\n",
    " - Percentage of female customers: 47.71%\n",
    "\n",
    "- Offer unsuccessful\n",
    " - Average customer income: 59179.6.7\n",
    " - Average customer age: 53.1 years\n",
    " - Percentage of male customers: 64.62%\n",
    " - Percentage of female customers: 35.37%\n",
    "  \n",
    "<b><u>Summary</u>:</b> 2015, 2016 and 2017 joined members are likely to opt for the offer, wheras for 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(2, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: ae264e3637204a6fb9bb56bc8210ddfd </b>\n",
    "\n",
    "- Offer successful\n",
    " - Average customer income: 68009.2\n",
    " - Average customer age: 56.2 years\n",
    " - Percentage of male customers: 50.35%\n",
    " - Percentage of female customers: 49.64%\n",
    " - Higher percentage of 2015 & 2016 membership start years\n",
    "\n",
    "- Offer unsuccessful\n",
    " - Average customer income: 58773.0\n",
    " - Average customer age: 52.6 years\n",
    " - Percentage of male customers: 68.15%\n",
    " - Percentage of female customers: 31.84%\n",
    " - Higher percentage of 2018 membership start year\n",
    " \n",
    "<b><u>Summary</u>:</b> 2015, 2016, 2017 joined members are likely to opt for the offer for 2018 offer adaption is high than other 3 offers, wheras for 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(3, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: 4d5c57ea9a6940dd891ad53e9dbe8da0 </b>\n",
    "\n",
    "- Offer successful\n",
    " - Average customer income: 69893.6\n",
    " - Average customer age: 56.7 years\n",
    " - Percentage of male customers: 51.57%\n",
    " - Percentage of female customers: 48.42%\n",
    "\n",
    "- Offer unsuccessful\n",
    " - Average customer income: 57751.5\n",
    " - Average customer age: 52.8 years\n",
    " - Percentage of male customers: 68.50%\n",
    " - Percentage of female customers: 31.49%\n",
    " \n",
    "<b><u>Summary</u>:</b> 2015, 2016, 2017 joined members are likely to opt for the offer, wheras for 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(4, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: 9b98b8c7a33c4b65b9aebfe6a799e6d9 </b>\n",
    "\n",
    "- Offer successful\n",
    " - Average customer income: 67761.2\n",
    " - Average customer age: 56.3 years\n",
    " - Percentage of male customers: 53.19%\n",
    " - Percentage of female customers: 46.80%\n",
    " \n",
    "- Offer unsuccessful\n",
    " - Average customer income: 59837.6\n",
    " - Average customer age: 53,6 years\n",
    " - Percentage of male customers: 63.84%\n",
    " - Percentage of female customers: 37.15%\n",
    " \n",
    "<b><u>Summary</u>:</b> 2015, 2016, 2017 joined members are likely to opt for the offer, wheras for 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(5, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: 2906b810c7d4411798c6938adc9daaa5 </b>\n",
    "\n",
    "- Offer successful\n",
    " - Average customer income: 66340.5\n",
    " - Average customer age: 55.8 years\n",
    " - Percentage of male customers: 52.10%\n",
    " - Percentage of female customers: 47.89%\n",
    "\n",
    "- Offer unsuccessful\n",
    " - Average customer income: 61272.3\n",
    " - Average customer age: 53.9 years\n",
    " - Percentage of male customers: 62.71%\n",
    " - Percentage of female customers: 37.28%\n",
    " \n",
    "<b><u>Summary</u>:</b> 2015, 2016, 2017 and 2018 joined members are likely to opt for the offer, wheras for 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(6, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: 0b1e1539f2cc45b7b9fa7c272da2e1d7 </b>\n",
    "\n",
    "- Offer successful\n",
    " - Average customer income: 67708.8\n",
    " - Average customer age: 56.3 years\n",
    " - Percentage of male customers: 52.45%\n",
    " - Percentage of female customers: 47.54%\n",
    "\n",
    "- Offer unsuccessful\n",
    " - Average customer income: 60124.7\n",
    " - Average customer age: 53.9 years\n",
    " - Percentage of male customers: 61.76%\n",
    " - Percentage of female customers: 38.23%\n",
    " \n",
    "<b><u>Summary</u>:</b> 2015, 2016, 2017 and 2018 joined members are likely to opt for the offer, wheras for 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(7, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: 3f207df678b143eea3cee63160fa8bed </b>\n",
    "\n",
    "- Offer successful\n",
    " - Average customer income: 67863.8\n",
    " - Average customer age: 57.3 years\n",
    " - Percentage of male customers: 50.14%\n",
    " - Percentage of male customers: 49.85%\n",
    "\n",
    "- Offer unsuccessful\n",
    " - Average customer income: 62980.9\n",
    " - Average customer age: 54.6 years\n",
    " - Percentage of male customers: 58.82%\n",
    " - Percentage of male customers: 41.17%\n",
    "\n",
    "<b><u>Summary</u>:</b> 2015, 2016, 2017 and 2018 joined members are likely to opt for the offer, wheras for 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(8, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> offerid: 5a8bc65990b245e5a138643cd4eb9837</b>\n",
    "\n",
    "- Offer successful\n",
    " - Average customer income: 64481.6\n",
    " - Average customer age: 56.8 years\n",
    " - Percentage of male customers: 57.43%\n",
    " - Percentage of female customers: 42.56%\n",
    "\n",
    "- Offer unsuccessful\n",
    " - Average customer income: 63835.8\n",
    " - Average customer age: 54.6 years\n",
    " - Percentage of male customers: 56.85%\n",
    " - Percentage of female customers: 43.14%\n",
    " \n",
    "<b><u>Summary</u>:</b> 2015, 2016, 2017 and 2018 joined members are likely to opt for the offer, wheras for 2015 and 2017 verdit is split and 2018 are more likely to reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_customer_offer(9, percent_success, training_data, gender_integer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling\n",
    "\n",
    "#### Evaluate naive predictor performance\n",
    "- A naive predictor assumes that all customer offers were successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_predictor_accuracy = accuracy_score(y_train,np.ones(len(y_train)))\n",
    "naive_predictor_f1score = f1_score(y_train, np.ones(len(y_train)))\n",
    "\n",
    "print(\"Naive predictor accuracy: %.3f\" % (naive_predictor_accuracy))\n",
    "print(\"Naive predictor f1-score: %.3f\" % (naive_predictor_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Naive predictor</b>\n",
    " - Accuracy: 0.469 & F1-score: 0.639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "lr_model_path = os.path.join(model_dir, 'lr_clf.joblib')\n",
    "\n",
    "scorer = make_scorer(fbeta_score,\n",
    "                     beta=0.5)\n",
    "\n",
    "if os.path.exists(lr_model_path):\n",
    "    lr_random = load(lr_model_path)\n",
    "else:\n",
    "    lr_clf = LogisticRegression(random_state=random_state,\n",
    "                                solver='liblinear')\n",
    "\n",
    "    random_grid = {'penalty': ['l1', 'l2'],\n",
    "                   'C': [1.0, 0.1, 0.01]}\n",
    "\n",
    "    lr_random = RandomizedSearchCV(estimator = lr_clf,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   scoring=scorer,\n",
    "                                   n_iter = 6,\n",
    "                                   cv = 3,\n",
    "                                   verbose=2,\n",
    "                                   random_state=random_state,\n",
    "                                   n_jobs = 3)\n",
    "\n",
    "    lr_random.fit(X_train, y_train)\n",
    "\n",
    "    dump(lr_random, lr_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Define model performance evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(clf,\n",
    "                               X_train,\n",
    "                               y_train):\n",
    "    \"\"\" Prints a model's accuracy and F1-score\n",
    "    \n",
    "    INPUT:\n",
    "        clf: Model object\n",
    "        \n",
    "        X_train: Training data matrix\n",
    "\n",
    "        y_train: Expected model output vector\n",
    "    \n",
    "    OUTPUT:\n",
    "        clf_accuracy: Model accuracy\n",
    "        \n",
    "        clf_f1_score: Model F1-score\"\"\"\n",
    "    class_name = re.sub(\"[<>']\", '', str(clf.__class__))\n",
    "    class_name = class_name.split(' ')[1]\n",
    "    class_name = class_name.split('.')[-1]\n",
    "\n",
    "    y_pred_rf = clf.predict(X_train)\n",
    "\n",
    "    clf_accuracy = accuracy_score(y_train, y_pred_rf)\n",
    "    clf_f1_score = f1_score(y_train, y_pred_rf)\n",
    "    \n",
    "    print(\"%s model accuracy: %.3f\" % (class_name, clf_accuracy))\n",
    "    print(\"%s model f1-score: %.3f\" % (class_name, clf_f1_score))\n",
    "    \n",
    "    return clf_accuracy, clf_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluate Logistic Regression Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(lr_random.best_estimator_,\n",
    "                           X_train,\n",
    "                           y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Construct Random Forest Model</b>\n",
    "\n",
    "- Perform random search of model hyperparameter space\n",
    "- Resuls suggest that a random forest model's accuracy and f1-score is better than the naive predictor\n",
    "\n",
    " - Accuracy\n",
    "   - Naive predictor: 0.469\n",
    "   - Logistic regression: 0.721\n",
    " - F1-score\n",
    "   - Naive predictor: 0.639\n",
    "   - Logistic regression: 0.716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_path = os.path.join(model_dir, 'rf_clf.joblib')\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "if os.path.exists(rf_model_path):\n",
    "    rf_random = load(rf_model_path)\n",
    "else:\n",
    "    rf_clf = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [10, 30, 50, 100, 150, 200, 250, 300]\n",
    "\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.arange(3, 11)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'bootstrap': [True, False],\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "    rf_random = RandomizedSearchCV(estimator = rf_clf,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   scoring=scorer,\n",
    "                                   n_iter = 100,\n",
    "                                   cv = 3,\n",
    "                                   verbose=2,\n",
    "                                   random_state=random_state,\n",
    "                                   n_jobs = 3)\n",
    "\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    \n",
    "    dump(rf_random, rf_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evalaute Random Forest Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(rf_random.best_estimator_,\n",
    "                           X_train,\n",
    "                           y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model Accuracy and F1 score </b>\n",
    "\n",
    "- Perform random search of model hyperparameter space\n",
    "- Resuls suggest that a random forest model's accuracy and f1-score is better than the naive predictor\n",
    "\n",
    " - Accuracy\n",
    "   - Naive predictor: 0.469\n",
    "   - Logistic regression: 0.721\n",
    "   - Random Forest: 0.736\n",
    " - F1-score\n",
    "   - Naive predictor: 0.639\n",
    "   - Logistic regression: 0.716\n",
    "   - Random Forest: 0.730"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Plot Estimated Feature Importance</b>\n",
    "\n",
    "* \"Feature importance\" refers to a numerical value that describes a feature's contribution to building a model that maximizes its evaluation metric\n",
    "\n",
    "* These results suggest that the top five features contibuting to 71.16% feature importance \n",
    " 1. Offer reward\n",
    " 2. Offer duration\n",
    " 3. Offer difficulty (how much money a customer must spend to complete an offer)\n",
    " 4. Membership year - specifically in the year 2018\n",
    " 5. Customer income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_importance = rf_random.best_estimator_.feature_importances_\n",
    "relative_importance = relative_importance / np.sum(relative_importance)\n",
    "\n",
    "feature_importance =\\\n",
    "    pd.DataFrame(list(zip(variable_names,\n",
    "                          relative_importance)),\n",
    "                 columns=['feature', 'relativeimportance'])\n",
    "\n",
    "feature_importance = feature_importance.sort_values('relativeimportance',\n",
    "                                                    ascending=False)\n",
    "\n",
    "feature_importance = feature_importance.reset_index(drop=True)\n",
    "\n",
    "palette = sns.color_palette(\"coolwarm\", feature_importance.shape[0])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.barplot(x='relativeimportance',\n",
    "            y='feature',\n",
    "            data=feature_importance,\n",
    "            palette=palette)\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Random Forest Estimated Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print the top 10 features sorted based on their estimated importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_path = os.path.join(model_dir, 'gb_clf.joblib')\n",
    "\n",
    "if os.path.exists(gb_model_path):\n",
    "    gb_random = load(gb_model_path)\n",
    "else:\n",
    "    gb_clf = GradientBoostingClassifier(random_state=random_state)\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'loss': ['deviance', 'exponential'],\n",
    "                   'learning_rate': [0.1, 0.01, 0.001],\n",
    "                   'n_estimators': [10, 30, 50, 100, 150, 200, 250, 300],\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'min_samples_split': min_samples_split}\n",
    "\n",
    "    gb_random = RandomizedSearchCV(estimator = gb_clf,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   scoring=scorer,\n",
    "                                   n_iter = 100,\n",
    "                                   cv = 3,\n",
    "                                   verbose=2,\n",
    "                                   random_state=random_state,\n",
    "                                   n_jobs = 3)\n",
    "\n",
    "    gb_random.fit(X_train, y_train)\n",
    "\n",
    "    dump(gb_random, gb_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluate Gradient Boosting Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(gb_random.best_estimator_,\n",
    "                           X_train,\n",
    "                           y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the best model\n",
    "* Model ranking based on training data accuracy\n",
    " * RandomForestClassifier model accuracy: 0.736\n",
    " * GradientBoostingClassifier model accuracy: 0.737\n",
    " * LogisticRegression model accuracy: 0.721\n",
    " * Naive predictor accuracy: 0.469\n",
    "\n",
    "* Model ranking based on training data F1-score\n",
    " * RandomForestClassifier model f1-score: 0.730\n",
    " * GradientBoostingClassifier model f1-score: 0.726\n",
    " * LogisticRegression model f1-score: 0.716\n",
    " * Naive predictor f1-score: 0.639\n",
    " \n",
    "* Results suggest that the random forest model and gradient boosting classifier are very close with .001 in favour of gradient boosting classifier whereas when looked in relation to F1-Score then best training data accuracy and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance are two characteristics of a machine learning model. \n",
    " - Bias refers to inherent model assumptions regarding the decision boundary between different classes. \n",
    " - On the other hand, variance refers a model's sensitivity to changes in its inputs. \n",
    "\n",
    "A logistic regression model constructs a linear decision boundary to separate successful and unsuccessful offers. However,  exploratory analysis of customer demographics for each offer suggests that this decision boundary will be non-linear. Therefore, an ensemble method like random forest or gradient boosting should perform better.\n",
    "\n",
    "Both random forest and gradient boosting models are a combination of multiple decision trees. A random forest classifier randomly samples the training data with replacement to construct a set of decision trees that are combined using majority voting. In contrast, gradient boosting iteratively constructs a set of decision trees with the goal of reducing the number of misclassified training data samples from the previous iteration. A consequence of these model construction strategies is that the depth of decision trees generated during random forest model training are typically greater than gradient boosting weak learner depth to minimize model variance. Typically, gradient boosting performs better than a random forest classifier. However, gradient boosting may overfit the training data and requires additional effort to tune. A random forest classifier is less prone to overfitting because it constructs decision trees from random training data samples. Also, a random forest classifier's hyperparameters are easier to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = []\n",
    "\n",
    "classifier_type = ['naivepredictor',\n",
    "                   'logisticregression',\n",
    "                   'randomforest',\n",
    "                   'gradientboosting']\n",
    "\n",
    "model_performance.append((naive_predictor_accuracy,\n",
    "                          naive_predictor_f1score))\n",
    "\n",
    "model_performance.append(evaluate_model_performance(lr_random.best_estimator_,\n",
    "                                                    X_train,\n",
    "                                                    y_train))\n",
    "\n",
    "model_performance.append(evaluate_model_performance(rf_random.best_estimator_,\n",
    "                                                    X_train,\n",
    "                                                    y_train))\n",
    "\n",
    "model_performance.append(evaluate_model_performance(gb_random.best_estimator_,\n",
    "                                                    X_train,\n",
    "                                                    y_train))\n",
    "\n",
    "model_performance = pd.DataFrame(model_performance,\n",
    "                                 columns=['accuracy', 'f1score'])\n",
    "\n",
    "classifier_type = pd.DataFrame(classifier_type,\n",
    "                               columns=['classifiertype'])\n",
    "\n",
    "model_performance = pd.concat([classifier_type, model_performance],\n",
    "                              axis=1)\n",
    "\n",
    "model_performance = model_performance.sort_values('accuracy', ascending=False)\n",
    "model_performance = model_performance.reset_index(drop=True)\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print the Best Model's Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Refine Best Model</b> \n",
    "\n",
    "* Refine model hyperparameter space\n",
    " * Hyperparameter Tuning the Random Forest in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best fit - 375 to 415 minutes to execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_fit_path = os.path.join(model_dir, 'grid_fit.joblib')\n",
    "\n",
    "if os.path.exists(grid_fit_path):\n",
    "    grid_fit = load(grid_fit_path)\n",
    "else:\n",
    "    parameters = {'n_estimators': [300, 350, 400, 450, 500],\n",
    "                  'max_depth': [10, 11, 12, 13, 14, 15],\n",
    "                  'min_samples_leaf': min_samples_leaf,\n",
    "                  'min_samples_split': min_samples_split,\n",
    "                  'random_state': [random_state]}\n",
    "\n",
    "    grid_obj = GridSearchCV(rf_clf,\n",
    "                            parameters,\n",
    "                            scoring=scorer,\n",
    "                            cv=5,\n",
    "                            n_jobs=3,\n",
    "                            verbose=2)\n",
    "\n",
    "    grid_fit = grid_obj.fit(X_train,\n",
    "                            y_train)\n",
    "\n",
    "    dump(grid_fit, grid_fit_path)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "evaluate_model_performance(best_clf,\n",
    "                           X_train,\n",
    "                           y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print the Refined Random Forest Model's Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluate Test Data Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(best_clf,\n",
    "                           X_test,\n",
    "                           y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "This analysis suggests that a random forest model has the best training data accuracy and F1-score. Refined random forest model hyperparameters using a grid search. Analysis suggests that the resulting random forest model has an training data accuracy of 0.753 and an F1-score of 0.746. The test data set accuracy of .735 and F1-score of .730 suggests that the random forest modeldid not overfit the training data.\n",
    "\n",
    "\"Feature importance\" refers to a numerical value that describes a feature's contribution to building a model that maximizes its evaluation metric. A random forest classifier is an example of a model that estimates feature importance during training.\n",
    "\n",
    "My analysis of the Starbucks Capstone Challenge customer offer effectiveness training data suggests that the top five features based on their importance are:\n",
    "1. Offer reward\n",
    "2. Offer duration\n",
    "3. Offer difficulty (how much money a customer must spend to complete an offer)\n",
    "4. Membership year - specifically in the year 2018\n",
    "5. Customer income  \n",
    "\n",
    "Since the top three features are associated with an customer offer, it is possible to further improve the performance of a random forest model by creating features that describe an offer's success rate as a function of offer difficulty, duration, and reward. These additional features should provide a random forest classifier the opportunity to construct a better decision boundary that separates successful and unsuccessful customer offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
